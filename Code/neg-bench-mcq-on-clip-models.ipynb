{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import re\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "from itertools import chain\n",
    "import clip\n",
    "import open_clip\n",
    "import tqdm\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MCQ Dataset - Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO shape: (5914, 7)\n",
      "VOC 2007 shape: (5031, 7)\n",
      "Final Metadata shape: (10945, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>caption_0</th>\n",
       "      <th>caption_1</th>\n",
       "      <th>caption_2</th>\n",
       "      <th>caption_3</th>\n",
       "      <th>correct_answer_template</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/coco/images/val2017/000000397133.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>This image features a knife, but no car is pre...</td>\n",
       "      <td>A car is present in this image, but there is n...</td>\n",
       "      <td>This image features a car</td>\n",
       "      <td>This image does not feature a knife.</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>coco</td>\n",
       "      <td>000000397133.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/coco/images/val2017/000000397133.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>A chair is not present in this image.</td>\n",
       "      <td>This image shows a chair, but no spoon is pres...</td>\n",
       "      <td>A chair is present in this image.</td>\n",
       "      <td>A spoon is not included in this image.</td>\n",
       "      <td>negative</td>\n",
       "      <td>coco</td>\n",
       "      <td>000000397133.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/coco/images/val2017/000000397133.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>A person is present in this image, but there's...</td>\n",
       "      <td>This image shows a fork, with no person in sight.</td>\n",
       "      <td>A fork is shown in this image.</td>\n",
       "      <td>No person is present in this image.</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>coco</td>\n",
       "      <td>000000397133.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  image_path  correct_answer  \\\n",
       "0  data/coco/images/val2017/000000397133.jpg               0   \n",
       "1  data/coco/images/val2017/000000397133.jpg               0   \n",
       "2  data/coco/images/val2017/000000397133.jpg               0   \n",
       "\n",
       "                                           caption_0  \\\n",
       "0  This image features a knife, but no car is pre...   \n",
       "1              A chair is not present in this image.   \n",
       "2  A person is present in this image, but there's...   \n",
       "\n",
       "                                           caption_1  \\\n",
       "0  A car is present in this image, but there is n...   \n",
       "1  This image shows a chair, but no spoon is pres...   \n",
       "2  This image shows a fork, with no person in sight.   \n",
       "\n",
       "                           caption_2                               caption_3  \\\n",
       "0          This image features a car    This image does not feature a knife.   \n",
       "1  A chair is present in this image.  A spoon is not included in this image.   \n",
       "2     A fork is shown in this image.     No person is present in this image.   \n",
       "\n",
       "  correct_answer_template dataset_name         file_name  \n",
       "0                  hybrid         coco  000000397133.jpg  \n",
       "1                negative         coco  000000397133.jpg  \n",
       "2                  hybrid         coco  000000397133.jpg  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco_mcq = '/workspace/Dataset/neg_bench/image_dataset_csv/COCO_val_mcq_llama3.1_rephrased.csv'\n",
    "voc_mcq = '/workspace/Dataset/neg_bench/image_dataset_csv/VOC2007_mcq_llama3.1_rephrased.csv'\n",
    "# synthetic_mcq = '/workspace/Dataset/neg_bench/image_dataset_csv/synthetic_mcq_llama3.1_rephrased.csv'\n",
    "# msr_vtt_mcq = '/Users/akanshagautam/Documents/MTech/Thesis/Dataset/NegClip/NegBench/evaluation data/videos/msr_vtt_mcq_rephrased_llama.csv'\n",
    "\n",
    "coco_df = pd.read_csv(coco_mcq)\n",
    "voc_df = pd.read_csv(voc_mcq)\n",
    "# synthetic_df = pd.read_csv(synthetic_mcq)\n",
    "# msr_vtt_df = pd.read_csv(msr_vtt_mcq)\n",
    "print(f\"COCO shape: {coco_df.shape}\")\n",
    "print(f\"VOC 2007 shape: {voc_df.shape}\")\n",
    "# print(f\"Synthetic shape: {synthetic_df.shape}\")\n",
    "# print(f\"MSR VTT shape: {msr_vtt_df.shape}\")\n",
    "\n",
    "metadata_caption_df = pd.concat([coco_df, voc_df], axis=0, ignore_index=True)\n",
    "print(f\"Final Metadata shape: {metadata_caption_df.shape}\")\n",
    "\n",
    "metadata_caption_df[['dataset_name', 'file_name']] = metadata_caption_df['image_path'].str.extract(r'data/([^/]+)/.*?/([^/]+\\.[a-z0-9]+)$')\n",
    "\n",
    "metadata_caption_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset_name\n",
       "coco       5914\n",
       "voc2007    5031\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_caption_df['dataset_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MCQ Dataset - Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO shape: (5000, 3)\n",
      "VOC shape: (4952, 3)\n",
      "Final Metadata shape: (9952, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>file_path</th>\n",
       "      <th>dataset_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000500826.jpg</td>\n",
       "      <td>/workspace/Dataset/neg_bench/coco_images/00000...</td>\n",
       "      <td>coco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000000126137.jpg</td>\n",
       "      <td>/workspace/Dataset/neg_bench/coco_images/00000...</td>\n",
       "      <td>coco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000000388258.jpg</td>\n",
       "      <td>/workspace/Dataset/neg_bench/coco_images/00000...</td>\n",
       "      <td>coco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000000329219.jpg</td>\n",
       "      <td>/workspace/Dataset/neg_bench/coco_images/00000...</td>\n",
       "      <td>coco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000000446005.jpg</td>\n",
       "      <td>/workspace/Dataset/neg_bench/coco_images/00000...</td>\n",
       "      <td>coco</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          file_name                                          file_path  \\\n",
       "0  000000500826.jpg  /workspace/Dataset/neg_bench/coco_images/00000...   \n",
       "1  000000126137.jpg  /workspace/Dataset/neg_bench/coco_images/00000...   \n",
       "2  000000388258.jpg  /workspace/Dataset/neg_bench/coco_images/00000...   \n",
       "3  000000329219.jpg  /workspace/Dataset/neg_bench/coco_images/00000...   \n",
       "4  000000446005.jpg  /workspace/Dataset/neg_bench/coco_images/00000...   \n",
       "\n",
       "  dataset_name  \n",
       "0         coco  \n",
       "1         coco  \n",
       "2         coco  \n",
       "3         coco  \n",
       "4         coco  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco_val_dir = '/workspace/Dataset/neg_bench/coco_images'\n",
    "# mediafire_val_dir = '/Users/akanshagautam/Documents/MTech/Thesis/Dataset/NegClip/mediafire_val_videos'\n",
    "voc_val_dir = '/workspace/Dataset/neg_bench/voc2007_images/VOC2007/JPEGImages'\n",
    "# synthetic_mcq = '/Users/akanshagautam/Documents/MTech/Thesis/Dataset/NegClip/NegBench/evaluation data/images/synthetic_mcq_llama3.1_rephrased.csv'\n",
    "\n",
    "coco_image_files = [f for f in os.listdir(coco_val_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "# mediafire_video_files = [f for f in os.listdir(mediafire_val_dir) if f.lower().endswith('.mp4')]\n",
    "voc_image_files = [f for f in os.listdir(voc_val_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "coco_val_df = pd.DataFrame({'file_name': coco_image_files, 'file_path': [os.path.join(coco_val_dir, f) for f in coco_image_files],'dataset_name': 'coco'})\n",
    "# mediafire_val_df = pd.DataFrame({'file_name': mediafire_video_files, 'file_path': [os.path.join(mediafire_val_dir, f) for f in mediafire_video_files], 'dataset_name': 'video'})\n",
    "voc_val_df = pd.DataFrame({'file_name': voc_image_files, 'file_path': [os.path.join(voc_val_dir, f) for f in voc_image_files], 'dataset_name': 'voc2007'})\n",
    "\n",
    "print(f\"COCO shape: {coco_val_df.shape}\")\n",
    "print(f\"VOC shape: {voc_val_df.shape}\")\n",
    "\n",
    "metadata_images_videos_df = pd.concat([coco_val_df, voc_val_df], axis=0, ignore_index=True)\n",
    "print(f\"Final Metadata shape: {metadata_images_videos_df.shape}\")\n",
    "\n",
    "metadata_images_videos_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the captions and images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged shape: (10945, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>caption_0</th>\n",
       "      <th>caption_1</th>\n",
       "      <th>caption_2</th>\n",
       "      <th>caption_3</th>\n",
       "      <th>correct_answer_template</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>file_name</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/coco/images/val2017/000000397133.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>This image features a knife, but no car is present.</td>\n",
       "      <td>A car is present in this image, but there is no knife.</td>\n",
       "      <td>This image features a car</td>\n",
       "      <td>This image does not feature a knife.</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>coco</td>\n",
       "      <td>000000397133.jpg</td>\n",
       "      <td>/workspace/Dataset/neg_bench/coco_images/000000397133.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/coco/images/val2017/000000397133.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>A chair is not present in this image.</td>\n",
       "      <td>This image shows a chair, but no spoon is present.</td>\n",
       "      <td>A chair is present in this image.</td>\n",
       "      <td>A spoon is not included in this image.</td>\n",
       "      <td>negative</td>\n",
       "      <td>coco</td>\n",
       "      <td>000000397133.jpg</td>\n",
       "      <td>/workspace/Dataset/neg_bench/coco_images/000000397133.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  image_path  correct_answer  \\\n",
       "0  data/coco/images/val2017/000000397133.jpg               0   \n",
       "1  data/coco/images/val2017/000000397133.jpg               0   \n",
       "\n",
       "                                             caption_0  \\\n",
       "0  This image features a knife, but no car is present.   \n",
       "1                A chair is not present in this image.   \n",
       "\n",
       "                                                caption_1  \\\n",
       "0  A car is present in this image, but there is no knife.   \n",
       "1      This image shows a chair, but no spoon is present.   \n",
       "\n",
       "                           caption_2                               caption_3  \\\n",
       "0          This image features a car    This image does not feature a knife.   \n",
       "1  A chair is present in this image.  A spoon is not included in this image.   \n",
       "\n",
       "  correct_answer_template dataset_name         file_name  \\\n",
       "0                  hybrid         coco  000000397133.jpg   \n",
       "1                negative         coco  000000397133.jpg   \n",
       "\n",
       "                                                   file_path  \n",
       "0  /workspace/Dataset/neg_bench/coco_images/000000397133.jpg  \n",
       "1  /workspace/Dataset/neg_bench/coco_images/000000397133.jpg  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.merge(metadata_caption_df,metadata_images_videos_df,on=['file_name', 'dataset_name'],how='inner')\n",
    "\n",
    "print(f\"Merged shape: {merged_df.shape}\")\n",
    "merged_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset into different negation types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_caption(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        \n",
    "        contractions = {\n",
    "            \"isn't\": \"isnot\", \"aren't\": \"arenot\",\"wasn't\": \"wasnot\",\"weren't\": \"werenot\",\"don't\": \"donot\",\"doesn't\": \"doesnot\",\n",
    "            \"didn't\": \"didnot\",\"can't\": \"cannot\",\"couldn't\": \"couldnot\",\"won't\": \"willnot\",\"wouldn't\": \"wouldnot\",\"shouldn't\": \"shouldnot\",\n",
    "            \"mustn't\": \"mustnot\",\"hadn't\": \"hadnot\",\"hasn't\": \"hasnot\",\"haven't\": \"havenot\",\"mightn't\": \"mightnot\",\n",
    "            \"needn't\": \"neednot\",\"shan't\": \"shallnot\",\n",
    "        }\n",
    "\n",
    "        for contraction, replacement in contractions.items():\n",
    "            text = text.replace(contraction, replacement)\n",
    "\n",
    "        text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
    "        text = text.strip()\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "merged_df['caption_0_clean'] = merged_df['caption_0'].apply(clean_caption)\n",
    "merged_df['caption_1_clean'] = merged_df['caption_1'].apply(clean_caption)\n",
    "merged_df['caption_2_clean'] = merged_df['caption_2'].apply(clean_caption)\n",
    "merged_df['caption_3_clean'] = merged_df['caption_3'].apply(clean_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>caption_0</th>\n",
       "      <th>caption_1</th>\n",
       "      <th>caption_2</th>\n",
       "      <th>caption_3</th>\n",
       "      <th>correct_answer_template</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>file_name</th>\n",
       "      <th>file_path</th>\n",
       "      <th>caption_0_clean</th>\n",
       "      <th>caption_1_clean</th>\n",
       "      <th>caption_2_clean</th>\n",
       "      <th>caption_3_clean</th>\n",
       "      <th>caption_0_negation_bucket</th>\n",
       "      <th>caption_1_negation_bucket</th>\n",
       "      <th>caption_2_negation_bucket</th>\n",
       "      <th>caption_3_negation_bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/coco/images/val2017/000000397133.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>This image features a knife, but no car is present.</td>\n",
       "      <td>A car is present in this image, but there is no knife.</td>\n",
       "      <td>This image features a car</td>\n",
       "      <td>This image does not feature a knife.</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>coco</td>\n",
       "      <td>000000397133.jpg</td>\n",
       "      <td>/workspace/Dataset/neg_bench/coco_images/000000397133.jpg</td>\n",
       "      <td>this image features a knife  but no car is present</td>\n",
       "      <td>a car is present in this image  but there is no knife</td>\n",
       "      <td>this image features a car</td>\n",
       "      <td>this image does not feature a knife</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>unknown/pragmatic</td>\n",
       "      <td>syntactic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/coco/images/val2017/000000397133.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>A chair is not present in this image.</td>\n",
       "      <td>This image shows a chair, but no spoon is present.</td>\n",
       "      <td>A chair is present in this image.</td>\n",
       "      <td>A spoon is not included in this image.</td>\n",
       "      <td>negative</td>\n",
       "      <td>coco</td>\n",
       "      <td>000000397133.jpg</td>\n",
       "      <td>/workspace/Dataset/neg_bench/coco_images/000000397133.jpg</td>\n",
       "      <td>a chair is not present in this image</td>\n",
       "      <td>this image shows a chair  but no spoon is present</td>\n",
       "      <td>a chair is present in this image</td>\n",
       "      <td>a spoon is not included in this image</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>unknown/pragmatic</td>\n",
       "      <td>syntactic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  image_path  correct_answer  \\\n",
       "0  data/coco/images/val2017/000000397133.jpg               0   \n",
       "1  data/coco/images/val2017/000000397133.jpg               0   \n",
       "\n",
       "                                             caption_0  \\\n",
       "0  This image features a knife, but no car is present.   \n",
       "1                A chair is not present in this image.   \n",
       "\n",
       "                                                caption_1  \\\n",
       "0  A car is present in this image, but there is no knife.   \n",
       "1      This image shows a chair, but no spoon is present.   \n",
       "\n",
       "                           caption_2                               caption_3  \\\n",
       "0          This image features a car    This image does not feature a knife.   \n",
       "1  A chair is present in this image.  A spoon is not included in this image.   \n",
       "\n",
       "  correct_answer_template dataset_name         file_name  \\\n",
       "0                  hybrid         coco  000000397133.jpg   \n",
       "1                negative         coco  000000397133.jpg   \n",
       "\n",
       "                                                   file_path  \\\n",
       "0  /workspace/Dataset/neg_bench/coco_images/000000397133.jpg   \n",
       "1  /workspace/Dataset/neg_bench/coco_images/000000397133.jpg   \n",
       "\n",
       "                                      caption_0_clean  \\\n",
       "0  this image features a knife  but no car is present   \n",
       "1                a chair is not present in this image   \n",
       "\n",
       "                                         caption_1_clean  \\\n",
       "0  a car is present in this image  but there is no knife   \n",
       "1      this image shows a chair  but no spoon is present   \n",
       "\n",
       "                    caption_2_clean                        caption_3_clean  \\\n",
       "0         this image features a car    this image does not feature a knife   \n",
       "1  a chair is present in this image  a spoon is not included in this image   \n",
       "\n",
       "  caption_0_negation_bucket caption_1_negation_bucket  \\\n",
       "0                 syntactic                 syntactic   \n",
       "1                 syntactic                 syntactic   \n",
       "\n",
       "  caption_2_negation_bucket caption_3_negation_bucket  \n",
       "0         unknown/pragmatic                 syntactic  \n",
       "1         unknown/pragmatic                 syntactic  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syntactic_negators = {\"no\", \"not\", \"never\", \"neither\", \"nobody\", \"nothing\"}\n",
    "ignore_words = {\"image\", \"images\", \"included\", \"include\", \"includes\", \"displays\", \n",
    "\"introduces\", \"information\", \"individual\", \"individuals\", \"insert\", \"image”\", \"improve\"}\n",
    "prefixes = (\"un\", \"dis\", \"in\", \"im\", \"ir\", \"il\", \"non\", \"mis\")\n",
    "lexical_negators = {\"without\", \"lack\", \"absent\", \"avoid\", \"missing\"}\n",
    "\n",
    "def is_syntactic_negation(caption):\n",
    "    tokens = caption.lower().split()\n",
    "    return any(word in syntactic_negators for word in tokens)\n",
    "\n",
    "def is_morphological_negation(caption):\n",
    "    words = caption.lower().split()\n",
    "    for word in words:\n",
    "        if word in ignore_words:\n",
    "            continue\n",
    "        if any(word.startswith(prefix) and len(word) > len(prefix) for prefix in prefixes):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_lexical_negation(caption):\n",
    "    tokens = caption.lower().split()\n",
    "    return any(word in lexical_negators for word in tokens)\n",
    "\n",
    "def classify_negation(caption):\n",
    "    if is_syntactic_negation(caption):\n",
    "        return \"syntactic\"\n",
    "    elif is_morphological_negation(caption):\n",
    "        return \"morphological\"\n",
    "    elif is_lexical_negation(caption):\n",
    "        return \"lexical/semantic\"\n",
    "    else:\n",
    "        return \"unknown/pragmatic\"\n",
    "    \n",
    "merged_df[\"caption_0_negation_bucket\"] = merged_df[\"caption_0_clean\"].apply(classify_negation)\n",
    "merged_df[\"caption_1_negation_bucket\"] = merged_df[\"caption_1_clean\"].apply(classify_negation)\n",
    "merged_df[\"caption_2_negation_bucket\"] = merged_df[\"caption_2_clean\"].apply(classify_negation)\n",
    "merged_df[\"caption_3_negation_bucket\"] = merged_df[\"caption_3_clean\"].apply(classify_negation)\n",
    "\n",
    "merged_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntactic: (10931, 18)\n",
      "Morphological: (43, 18)\n",
      "Lexical/Semantic: (814, 18)\n"
     ]
    }
   ],
   "source": [
    "# Split dataset\n",
    "negation_cols = [ 'caption_0_negation_bucket', 'caption_1_negation_bucket', 'caption_2_negation_bucket', 'caption_3_negation_bucket']\n",
    "\n",
    "syntactic_df = merged_df[merged_df[negation_cols].apply(lambda x: x.astype(str).str.contains(\"syntactic\", case=False).any(), axis=1)]\n",
    "morphological_df = merged_df[merged_df[negation_cols].apply(lambda x: x.astype(str).str.contains(\"morphological\", case=False).any(), axis=1)]\n",
    "lexical_semantic_df = merged_df[merged_df[negation_cols].apply(lambda x: x.astype(str).str.contains(\"lexical/semantic\", case=False, regex=True).any(), axis=1)]\n",
    "\n",
    "print(f\"Syntactic: {syntactic_df.shape}\")\n",
    "print(f\"Morphological: {morphological_df.shape}\")\n",
    "print(f\"Lexical/Semantic: {lexical_semantic_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run models on all the split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_mcq_accuracy(df, model, processor, device):\n",
    "    \"\"\"\n",
    "    Evaluate MCQ accuracy on negation understanding using CLIP with GPU.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with 'file_path', 'caption_0' to 'caption_3', and 'correct_caption_index'.\n",
    "        model: CLIPModel instance on GPU.\n",
    "        processor: CLIPProcessor for input preprocessing.\n",
    "        device: torch.device ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        results_df (pd.DataFrame): Results with predicted indices.\n",
    "        accuracy (float): Accuracy score over the dataset.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    correct_flags = []\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Evaluating MCQ\"):\n",
    "        image = Image.open(row[\"file_path\"]).convert(\"RGB\")\n",
    "        captions = [row[f\"caption_{i}\"] for i in range(4)]\n",
    "        correct_index = row[\"correct_answer\"]\n",
    "\n",
    "        # Preprocess and move to device\n",
    "        inputs = processor(text=captions, images=[image]*4, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            similarities = torch.nn.functional.cosine_similarity(\n",
    "                outputs.image_embeds, outputs.text_embeds\n",
    "            )\n",
    "\n",
    "        pred_index = similarities.argmax().item()\n",
    "        predictions.append(pred_index)\n",
    "        correct_flags.append(int(pred_index == correct_index))\n",
    "\n",
    "    # Results and accuracy\n",
    "    results_df = df.copy()\n",
    "    results_df[\"predicted_index\"] = predictions\n",
    "    results_df[\"correct_prediction\"] = correct_flags\n",
    "    accuracy = sum(correct_flags) / len(correct_flags)\n",
    "\n",
    "    return results_df, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mcq_accuracy_v2(df, model, preprocess, tokenizer, device):\n",
    "    \"\"\"\n",
    "    Evaluate MCQ accuracy for OpenCLIP-based model on negation-sensitive MCQ dataset.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with 'file_path', 'caption_0' to 'caption_3', and 'correct_answer'.\n",
    "        model: OpenCLIP model.\n",
    "        preprocess: Image preprocessing function.\n",
    "        tokenizer: Tokenizer for text.\n",
    "        device: torch.device.\n",
    "\n",
    "    Returns:\n",
    "        results_df (pd.DataFrame): DataFrame with predictions and correctness.\n",
    "        accuracy (float): Proportion of correct predictions.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    correct_flags = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Evaluating MCQ\"):\n",
    "        image = preprocess(Image.open(row[\"file_path\"]).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "        captions = [row[f\"caption_{i}\"] for i in range(4)]\n",
    "        correct_index = row[\"correct_answer\"]\n",
    "\n",
    "        # Repeat image 4 times for each caption\n",
    "        image_batch = image.repeat(4, 1, 1, 1)\n",
    "\n",
    "        # Tokenize captions\n",
    "        text_tokens = tokenizer(captions).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image_batch)\n",
    "            text_features = model.encode_text(text_tokens)\n",
    "\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            similarity = (image_features * text_features).sum(dim=-1)  # Dot product\n",
    "\n",
    "        pred_index = similarity.argmax().item()\n",
    "        predictions.append(pred_index)\n",
    "        correct_flags.append(int(pred_index == correct_index))\n",
    "\n",
    "    results_df = df.copy()\n",
    "    results_df[\"predicted_index\"] = predictions\n",
    "    results_df[\"correct_prediction\"] = correct_flags\n",
    "    accuracy = sum(correct_flags) / len(correct_flags)\n",
    "\n",
    "    return results_df, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mcq_accuracy_v3(df, model, preprocess, device):\n",
    "    \"\"\"\n",
    "    Evaluate MCQ accuracy using OpenAI-style CLIP with a custom checkpoint.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Must contain 'file_path', 'caption_0' to 'caption_3', and 'correct_answer'.\n",
    "        model: CLIP model (from clip.load or ConCLIP).\n",
    "        preprocess: Image preprocessing transform (from clip.load).\n",
    "        device: 'cuda' or 'cpu'.\n",
    "\n",
    "    Returns:\n",
    "        results_df (pd.DataFrame), accuracy (float)\n",
    "    \"\"\"\n",
    "    import clip  # Make sure the clip package is imported\n",
    "\n",
    "    predictions = []\n",
    "    correct_flags = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Evaluating MCQ\"):\n",
    "        image = preprocess(Image.open(row[\"file_path\"]).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "        captions = [row[f\"caption_{i}\"] for i in range(4)]\n",
    "        correct_index = row[\"correct_answer\"]\n",
    "\n",
    "        # Repeat image to match number of captions\n",
    "        image_input = image.repeat(len(captions), 1, 1, 1)  # Shape: [4, 3, H, W]\n",
    "        text_tokens = clip.tokenize(captions).to(device)   # Shape: [4, 77]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image_input)\n",
    "            text_features = model.encode_text(text_tokens)\n",
    "\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            similarities = (image_features * text_features).sum(dim=-1)  # Dot product = cosine similarity\n",
    "\n",
    "        pred_index = similarities.argmax().item()\n",
    "        predictions.append(pred_index)\n",
    "        correct_flags.append(int(pred_index == correct_index))\n",
    "\n",
    "    results_df = df.copy()\n",
    "    results_df[\"predicted_index\"] = predictions\n",
    "    results_df[\"correct_prediction\"] = correct_flags\n",
    "    accuracy = sum(correct_flags) / len(correct_flags)\n",
    "\n",
    "    return results_df, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clip_with_custom_checkpoint(model_name: str, checkpoint_path: str, device: str = \"cuda\"):\n",
    "    \"\"\"\n",
    "    Load CLIP model with a custom checkpoint.\n",
    "    \"\"\"\n",
    "    model, preprocess = clip.load(model_name, device=device)\n",
    "    ckpt = torch.load(checkpoint_path, weights_only=False)\n",
    "    model = model.float()\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    model = model.to(device)\n",
    "    return model.eval(), preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_negclip_checkpoint(model_name: str, checkpoint_path: str, device: str = \"cuda\"):\n",
    "    model, preprocess = clip.load(model_name, device=device)\n",
    "\n",
    "    # Load checkpoint\n",
    "    ckpt = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    state_dict = ckpt[\"state_dict\"]  # extract the actual model weights\n",
    "\n",
    "    # Clean up keys (remove any \"model.\" or \"module.\" prefixes)\n",
    "    cleaned_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        new_key = k\n",
    "        if k.startswith(\"model.\"):\n",
    "            new_key = k[len(\"model.\"):]\n",
    "        elif k.startswith(\"module.\"):\n",
    "            new_key = k[len(\"module.\"):]\n",
    "        cleaned_state_dict[new_key] = v\n",
    "\n",
    "    # Load weights\n",
    "    model.load_state_dict(cleaned_state_dict, strict=False)  # use strict=False to skip harmless mismatches\n",
    "    model = model.to(device)\n",
    "    return model.eval(), preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clip ViT Base Patch32 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MCQ: 100%|██████████| 10931/10931 [07:23<00:00, 24.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negation Score on syntactic dataset: 0.3900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MCQ: 100%|██████████| 814/814 [00:33<00:00, 24.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negation Score on lexical/semantic dataset: 0.3378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MCQ: 100%|██████████| 43/43 [00:01<00:00, 26.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negation Score on morphological dataset: 0.4186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_df, score = evaluate_mcq_accuracy(syntactic_df, model, processor, device)\n",
    "print(f\"Negation Score on syntactic dataset: {score:.4f}\")\n",
    "\n",
    "results_df, score = evaluate_mcq_accuracy(lexical_semantic_df, model, processor, device)\n",
    "print(f\"Negation Score on lexical/semantic dataset: {score:.4f}\")\n",
    "\n",
    "results_df, score = evaluate_mcq_accuracy(morphological_df, model, processor, device)\n",
    "print(f\"Negation Score on morphological dataset: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clip ViT Base Patch16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MCQ: 100%|██████████| 10931/10931 [07:51<00:00, 23.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negation Score on syntactic dataset: 0.3990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MCQ: 100%|██████████| 814/814 [00:35<00:00, 23.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negation Score on lexical/semantic dataset: 0.3575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MCQ: 100%|██████████| 43/43 [00:01<00:00, 24.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negation Score on morphological dataset: 0.4651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_df, score = evaluate_mcq_accuracy(syntactic_df, model, processor, device)\n",
    "print(f\"\\nNegation Score on syntactic dataset: {score:.4f}\")\n",
    "\n",
    "results_df, score = evaluate_mcq_accuracy(lexical_semantic_df, model, processor, device)\n",
    "print(f\"\\nNegation Score on lexical/semantic dataset: {score:.4f}\")\n",
    "\n",
    "results_df, score = evaluate_mcq_accuracy(morphological_df, model, processor, device)\n",
    "print(f\"\\nNegation Score on morphological dataset: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clip ViT Large Patch14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MCQ: 100%|██████████| 10931/10931 [18:06<00:00, 10.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negation Score on syntactic dataset: 0.3852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MCQ: 100%|██████████| 814/814 [01:21<00:00, 10.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negation Score on lexical/semantic dataset: 0.3305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MCQ: 100%|██████████| 43/43 [00:04<00:00, 10.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negation Score on morphological dataset: 0.4651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_df, score = evaluate_mcq_accuracy(syntactic_df, model, processor, device)\n",
    "print(f\"\\nNegation Score on syntactic dataset: {score:.4f}\")\n",
    "\n",
    "results_df, score = evaluate_mcq_accuracy(lexical_semantic_df, model, processor, device)\n",
    "print(f\"\\nNegation Score on lexical/semantic dataset: {score:.4f}\")\n",
    "\n",
    "results_df, score = evaluate_mcq_accuracy(morphological_df, model, processor, device)\n",
    "print(f\"\\nNegation Score on morphological dataset: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAION CLIP-ViT-H-14-laion2B-s32B-b79K model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "    (patch_dropout): Identity()\n",
       "    (ln_pre): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-31): 32 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0-23): 24 x ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 1024)\n",
       "  (ln_final): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"ViT-H-14\"\n",
    "pretrained = \"laion2B-s32B-b79K\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(model_name=model_name,pretrained=pretrained,device=device)\n",
    "\n",
    "tokenizer = open_clip.get_tokenizer(model_name)\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MCQ: 100%|██████████| 10931/10931 [31:03<00:00,  5.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negation Score on syntactic dataset: 0.3110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MCQ: 100%|██████████| 814/814 [02:19<00:00,  5.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negation Score on lexical/semantic dataset: 0.2322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MCQ: 100%|██████████| 43/43 [00:07<00:00,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negation Score on morphological dataset: 0.4884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_df, score = evaluate_mcq_accuracy_v2(syntactic_df, model, preprocess, tokenizer, device)\n",
    "print(f\"\\nNegation Score on syntactic dataset: {score:.4f}\")\n",
    "\n",
    "results_df, score = evaluate_mcq_accuracy_v2(lexical_semantic_df, model, preprocess, tokenizer, device)\n",
    "print(f\"\\nNegation Score on lexical/semantic dataset: {score:.4f}\")\n",
    "\n",
    "results_df, score = evaluate_mcq_accuracy_v2(morphological_df, model, preprocess, tokenizer, device)\n",
    "print(f\"\\nNegation Score on morphological dataset: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoN-CLIP ViT-B/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"/workspace/Models/conclip/conclip_vit_b32.pt\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"ViT-B/32\"\n",
    "\n",
    "model, preprocess = load_clip_with_custom_checkpoint(model_name, checkpoint_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MCQ: 100%|██████████| 10931/10931 [06:47<00:00, 26.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negation Score on syntactic dataset: 0.3051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MCQ: 100%|██████████| 814/814 [00:30<00:00, 26.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negation Score on lexical/semantic dataset: 0.2236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MCQ: 100%|██████████| 43/43 [00:01<00:00, 28.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negation Score on morphological dataset: 0.2558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_df, score = evaluate_mcq_accuracy_v3(syntactic_df, model, preprocess, device)\n",
    "print(f\"\\nNegation Score on syntactic dataset: {score:.4f}\")\n",
    "\n",
    "results_df, score = evaluate_mcq_accuracy_v3(lexical_semantic_df, model, preprocess, device)\n",
    "print(f\"\\nNegation Score on lexical/semantic dataset: {score:.4f}\")\n",
    "\n",
    "results_df, score = evaluate_mcq_accuracy_v3(morphological_df, model, preprocess, device)\n",
    "print(f\"\\nNegation Score on morphological dataset: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CON-CLIP ViT-B/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 335M/335M [00:04<00:00, 77.3MiB/s]\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"/workspace/Models/conclip/conclip_vit_b16.pt\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"ViT-B/16\"\n",
    "\n",
    "model, preprocess = load_clip_with_custom_checkpoint(model_name, checkpoint_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MCQ: 100%|██████████| 10931/10931 [07:05<00:00, 25.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negation Score on syntactic dataset: 0.2991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MCQ: 100%|██████████| 814/814 [00:30<00:00, 26.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negation Score on lexical/semantic dataset: 0.2555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MCQ: 100%|██████████| 43/43 [00:01<00:00, 24.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negation Score on morphological dataset: 0.2791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_df, score = evaluate_mcq_accuracy_v3(syntactic_df, model, preprocess, device)\n",
    "print(f\"\\nNegation Score on syntactic dataset: {score:.4f}\")\n",
    "\n",
    "results_df, score = evaluate_mcq_accuracy_v3(lexical_semantic_df, model, preprocess, device)\n",
    "print(f\"\\nNegation Score on lexical/semantic dataset: {score:.4f}\")\n",
    "\n",
    "results_df, score = evaluate_mcq_accuracy_v3(morphological_df, model, preprocess, device)\n",
    "print(f\"\\nNegation Score on morphological dataset: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CON-CLIP ViT-L/14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 890M/890M [00:31<00:00, 29.4MiB/s]\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"/workspace/Models/conclip/conclip_vit_l14.pt\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"ViT-L/14\"\n",
    "\n",
    "model, preprocess = load_clip_with_custom_checkpoint(model_name, checkpoint_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MCQ: 100%|██████████| 10931/10931 [16:23<00:00, 11.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negation Score on syntactic dataset: 0.3365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MCQ: 100%|██████████| 814/814 [01:13<00:00, 11.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negation Score on lexical/semantic dataset: 0.2531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MCQ: 100%|██████████| 43/43 [00:03<00:00, 11.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negation Score on morphological dataset: 0.3023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_df, score = evaluate_mcq_accuracy_v3(syntactic_df, model, preprocess, device)\n",
    "print(f\"\\nNegation Score on syntactic dataset: {score:.4f}\")\n",
    "\n",
    "results_df, score = evaluate_mcq_accuracy_v3(lexical_semantic_df, model, preprocess, device)\n",
    "print(f\"\\nNegation Score on lexical/semantic dataset: {score:.4f}\")\n",
    "\n",
    "results_df, score = evaluate_mcq_accuracy_v3(morphological_df, model, preprocess, device)\n",
    "print(f\"\\nNegation Score on morphological dataset: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NegClip ICLR 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"/workspace/Models/negclip/negclip_iclr_2023.pth\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"ViT-B/32\"\n",
    "\n",
    "model, preprocess = load_negclip_checkpoint(model_name, checkpoint_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MCQ: 100%|██████████| 10931/10931 [07:01<00:00, 25.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negation Score on syntactic dataset: 0.2876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MCQ: 100%|██████████| 814/814 [00:30<00:00, 27.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negation Score on lexical/semantic dataset: 0.2187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MCQ: 100%|██████████| 43/43 [00:01<00:00, 24.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negation Score on morphological dataset: 0.3488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_df, score = evaluate_mcq_accuracy_v3(syntactic_df, model, preprocess, device)\n",
    "print(f\"\\nNegation Score on syntactic dataset: {score:.4f}\")\n",
    "\n",
    "results_df, score = evaluate_mcq_accuracy_v3(lexical_semantic_df, model, preprocess, device)\n",
    "print(f\"\\nNegation Score on lexical/semantic dataset: {score:.4f}\")\n",
    "\n",
    "results_df, score = evaluate_mcq_accuracy_v3(morphological_df, model, preprocess, device)\n",
    "print(f\"\\nNegation Score on morphological dataset: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clip CC12M Negfull ViT-B/32 LR-1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"/workspace/Models/negclip/clip_cc12m_negfull_vit_b_32_lr1e-8_clw_0_99_mlw_0_01.pt\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"ViT-B/32\"\n",
    "\n",
    "model, preprocess = load_negclip_checkpoint(model_name, checkpoint_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MCQ: 100%|██████████| 10931/10931 [06:53<00:00, 26.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negation Score on syntactic dataset: 0.5786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MCQ: 100%|██████████| 814/814 [00:30<00:00, 26.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negation Score on lexical/semantic dataset: 0.5283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MCQ: 100%|██████████| 43/43 [00:01<00:00, 26.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negation Score on morphological dataset: 0.5814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_df, score = evaluate_mcq_accuracy_v3(syntactic_df, model, preprocess, device)\n",
    "print(f\"\\nNegation Score on syntactic dataset: {score:.4f}\")\n",
    "\n",
    "results_df, score = evaluate_mcq_accuracy_v3(lexical_semantic_df, model, preprocess, device)\n",
    "print(f\"\\nNegation Score on lexical/semantic dataset: {score:.4f}\")\n",
    "\n",
    "results_df, score = evaluate_mcq_accuracy_v3(morphological_df, model, preprocess, device)\n",
    "print(f\"\\nNegation Score on morphological dataset: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negclip CC12M Negfull ViT-B/32 LR-1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"/workspace/Models/negclip/negclip_cc12m_negfull_vit_b_32_lr1e-8_clw_0_99_mlw_0_01.pt\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"ViT-B/32\"\n",
    "\n",
    "model, preprocess = load_negclip_checkpoint(model_name, checkpoint_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MCQ: 100%|██████████| 10931/10931 [06:52<00:00, 26.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negation Score on syntactic dataset: 0.5795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MCQ: 100%|██████████| 814/814 [00:30<00:00, 26.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negation Score on lexical/semantic dataset: 0.5246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MCQ: 100%|██████████| 43/43 [00:01<00:00, 33.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negation Score on morphological dataset: 0.6744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_df, score = evaluate_mcq_accuracy_v3(syntactic_df, model, preprocess, device)\n",
    "print(f\"\\nNegation Score on syntactic dataset: {score:.4f}\")\n",
    "\n",
    "results_df, score = evaluate_mcq_accuracy_v3(lexical_semantic_df, model, preprocess, device)\n",
    "print(f\"\\nNegation Score on lexical/semantic dataset: {score:.4f}\")\n",
    "\n",
    "results_df, score = evaluate_mcq_accuracy_v3(morphological_df, model, preprocess, device)\n",
    "print(f\"\\nNegation Score on morphological dataset: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP-ViT-B-32-DataComp.S-s13M-b4K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (patch_dropout): Identity()\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-11): 12 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0-11): 12 x ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:laion/CLIP-ViT-B-32-DataComp.S-s13M-b4K')\n",
    "tokenizer = open_clip.get_tokenizer('hf-hub:laion/CLIP-ViT-B-32-DataComp.S-s13M-b4K')\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MCQ: 100%|██████████| 10931/10931 [06:37<00:00, 27.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negation Score on syntactic dataset: 0.2658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MCQ: 100%|██████████| 814/814 [00:30<00:00, 26.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negation Score on lexical/semantic dataset: 0.2432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MCQ: 100%|██████████| 43/43 [00:01<00:00, 28.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Negation Score on morphological dataset: 0.3256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_df, score = evaluate_mcq_accuracy_v3(syntactic_df, model, preprocess, device)\n",
    "print(f\"\\nNegation Score on syntactic dataset: {score:.4f}\")\n",
    "\n",
    "results_df, score = evaluate_mcq_accuracy_v3(lexical_semantic_df, model, preprocess, device)\n",
    "print(f\"\\nNegation Score on lexical/semantic dataset: {score:.4f}\")\n",
    "\n",
    "results_df, score = evaluate_mcq_accuracy_v3(morphological_df, model, preprocess, device)\n",
    "print(f\"\\nNegation Score on morphological dataset: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
