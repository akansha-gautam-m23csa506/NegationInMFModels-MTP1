{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import re\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from PIL import Image\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"max_colwidth\", None)\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load images metadata to a df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(302123, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000318123.jpg</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000318123.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000275481.jpg</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000275481.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000204278.jpg</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000204278.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000139141.jpg</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000139141.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000136272.jpg</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000136272.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      image_name  \\\n",
       "0  000318123.jpg   \n",
       "1  000275481.jpg   \n",
       "2  000204278.jpg   \n",
       "3  000139141.jpg   \n",
       "4  000136272.jpg   \n",
       "\n",
       "                                                                                                                  image_path  \n",
       "0  /Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000318123.jpg  \n",
       "1  /Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000275481.jpg  \n",
       "2  /Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000204278.jpg  \n",
       "3  /Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000139141.jpg  \n",
       "4  /Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000136272.jpg  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dir = \"/Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final\"\n",
    "\n",
    "image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff', '.webp'}\n",
    "\n",
    "image_data = []\n",
    "for root, _, files in os.walk(image_dir):\n",
    "    for file in files:\n",
    "        if os.path.splitext(file)[1].lower() in image_extensions:\n",
    "            image_path = os.path.join(root, file)\n",
    "            image_data.append({'image_name': file, 'image_path': image_path})\n",
    "\n",
    "images_metadata_df = pd.DataFrame(image_data)\n",
    "print(images_metadata_df.shape)\n",
    "images_metadata_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load captions to a df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: ['kept', 'dropped', 'image_paths', 'annotations', 'num_ops']\n"
     ]
    }
   ],
   "source": [
    "ccneg_preprocessed_data = torch.load(\"/Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_preprocessed.pt\")\n",
    "\n",
    "print(f\"Keys: {list(ccneg_preprocessed_data.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(228246, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_number</th>\n",
       "      <th>file_extension</th>\n",
       "      <th>true_caption</th>\n",
       "      <th>labels</th>\n",
       "      <th>negated_caption</th>\n",
       "      <th>url</th>\n",
       "      <th>subject</th>\n",
       "      <th>object_predicate_pairs</th>\n",
       "      <th>predicate</th>\n",
       "      <th>negate_word_present</th>\n",
       "      <th>num_ops</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000003</td>\n",
       "      <td>jpg</td>\n",
       "      <td>actor attends the season premiere</td>\n",
       "      <td>musician,premiere,event,singer,suit,performance</td>\n",
       "      <td>actor, not attending the season premiere</td>\n",
       "      <td>https://media.gettyimages.com/photos/aidan-gillen-attends-the-season-7-premiere-of-hbos-game-of-thrones-at-picture-id817717986?s=612x612</td>\n",
       "      <td>actor</td>\n",
       "      <td>[{'object': 'season premiere', 'predicate': 'attends the'}]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_number file_extension                       true_caption  \\\n",
       "0    000000003            jpg  actor attends the season premiere   \n",
       "\n",
       "                                            labels  \\\n",
       "0  musician,premiere,event,singer,suit,performance   \n",
       "\n",
       "                            negated_caption  \\\n",
       "0  actor, not attending the season premiere   \n",
       "\n",
       "                                                                                                                                        url  \\\n",
       "0  https://media.gettyimages.com/photos/aidan-gillen-attends-the-season-7-premiere-of-hbos-game-of-thrones-at-picture-id817717986?s=612x612   \n",
       "\n",
       "  subject                                       object_predicate_pairs  \\\n",
       "0   actor  [{'object': 'season premiere', 'predicate': 'attends the'}]   \n",
       "\n",
       "  predicate  negate_word_present  num_ops  \n",
       "0      None                 True        1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations = ccneg_preprocessed_data['annotations']\n",
    "\n",
    "rows = []\n",
    "\n",
    "for i in range(len(annotations)):\n",
    "    ann = annotations[i]\n",
    "\n",
    "    row = {\n",
    "        'image_number': ann.get('image_number'),\n",
    "        'file_extension':  ann.get('file_extension'),\n",
    "        'true_caption': ann.get('caption'),\n",
    "        'labels': ann.get('labels'),\n",
    "        'negated_caption': ann.get('sop_data', {}).get('negative-prompt'),\n",
    "        'url': ann.get('json', {}).get('url'),\n",
    "        'subject': ann.get('sop_data', {}).get('sop_decomposition', {}).get('subject'),\n",
    "        'object_predicate_pairs': ann.get('sop_data', {}).get('sop_decomposition', {}).get('object-predicate-pairs', {}),\n",
    "        'predicate': ann.get('sop_data', {}).get('sop_decomposition', {}).get('predicate'),\n",
    "        'negate_word_present': ann.get('negate_word_present'),\n",
    "        'num_ops': ann.get('num_ops')\n",
    "    }\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "annotiation_df = pd.DataFrame(rows)\n",
    "print(annotiation_df.shape)\n",
    "annotiation_df.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(228246, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_number</th>\n",
       "      <th>file_extension</th>\n",
       "      <th>true_caption</th>\n",
       "      <th>labels</th>\n",
       "      <th>negated_caption</th>\n",
       "      <th>url</th>\n",
       "      <th>subject</th>\n",
       "      <th>object_predicate_pairs</th>\n",
       "      <th>predicate</th>\n",
       "      <th>negate_word_present</th>\n",
       "      <th>num_ops</th>\n",
       "      <th>image_name</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000003</td>\n",
       "      <td>jpg</td>\n",
       "      <td>actor attends the season premiere</td>\n",
       "      <td>musician,premiere,event,singer,suit,performance</td>\n",
       "      <td>actor, not attending the season premiere</td>\n",
       "      <td>https://media.gettyimages.com/photos/aidan-gillen-attends-the-season-7-premiere-of-hbos-game-of-thrones-at-picture-id817717986?s=612x612</td>\n",
       "      <td>actor</td>\n",
       "      <td>[{'object': 'season premiere', 'predicate': 'attends the'}]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>000000003.jpg</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000000003.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000000005</td>\n",
       "      <td>jpg</td>\n",
       "      <td>a woman walks her dog on the beach .</td>\n",
       "      <td>water,beach,sea,shore,ocean,canidae,dog,sky,wave,coast,mudflat,dog walking,human,sand,photography</td>\n",
       "      <td>a woman walks on the beach without her dog</td>\n",
       "      <td>https://media.gettyimages.com/photos/woman-walks-her-dog-on-the-beach-on-october-21-2014-in-saltcoats-picture-id457587968</td>\n",
       "      <td>woman</td>\n",
       "      <td>[{'object': 'dog', 'predicate': 'walks her on the beach'}]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>000000005.jpg</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000000005.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000000009</td>\n",
       "      <td>jpg</td>\n",
       "      <td>close up portrait of a smiling middle aged woman sitting against white wall</td>\n",
       "      <td>hair,sitting,facial expression,nose,arm,cheek,smile,chin,lip,hand,close-up,brown hair,finger,wool,fur</td>\n",
       "      <td>close up portrait of a smiling middle aged woman without white wall</td>\n",
       "      <td>http://l7.alamy.com/zooms/29164f933d7340be90af1ab4c91f3644/close-up-portrait-of-a-smiling-middle-aged-woman-sitting-against-white-hxeygn.jpg</td>\n",
       "      <td>portrait</td>\n",
       "      <td>[{'object': 'woman', 'predicate': 'of a'}, {'object': 'wall', 'predicate': 'against white'}, {'object': 'age', 'predicate': 'middle'}, {'object': 'smile', 'predicate': 'smiling'}]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>000000009.jpg</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000000009.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000000018</td>\n",
       "      <td>jpg</td>\n",
       "      <td>man sitting on floor beside a pool using laptop</td>\n",
       "      <td>sitting,tablet computer,table,technology,grass,drinking,swimming pool,leisure,furniture,gadget,electronic device,vacation,laptop,stock photography,computer</td>\n",
       "      <td>man sitting on floor without a pool using laptop</td>\n",
       "      <td>https://media.gettyimages.com/photos/man-sitting-on-floor-beside-a-pool-using-laptop-picture-id588490995?s=612x612</td>\n",
       "      <td>man</td>\n",
       "      <td>[{'object': 'floor', 'predicate': 'on'}, {'object': 'pool', 'predicate': 'beside a'}, {'object': 'laptop', 'predicate': 'using'}]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>000000018.jpg</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000000018.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000000007</td>\n",
       "      <td>jpg</td>\n",
       "      <td>a beautiful day with some buildings and plants .</td>\n",
       "      <td>residential area,property,home,building,house,real estate,neighbourhood,town,architecture,estate,suburb,driveway,tree,road surface,road</td>\n",
       "      <td>a beautiful day without plants</td>\n",
       "      <td>https://d1tq208oegmb9e.cloudfront.net/site_photos_image/dbx%3A/urban+project/orange+county/fullerton/chapman+villas/Photos/3.jpg</td>\n",
       "      <td>day</td>\n",
       "      <td>[{'object': 'buildings', 'predicate': 'with some'}, {'object': 'plants', 'predicate': 'with some'}]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>000000007.jpg</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000000007.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>000000000</td>\n",
       "      <td>jpg</td>\n",
       "      <td>christmas tree on a black background .</td>\n",
       "      <td>christmas tree,christmas decoration,font,text,graphic design,illustration,interior design,tree,christmas eve,ornament,fir,plant,pine,pine family,graphics</td>\n",
       "      <td>christmas tree, not on a black background</td>\n",
       "      <td>https://thumb1.shutterstock.com/display_pic_with_logo/261388/223876810/stock-vector-christmas-tree-on-a-black-background-vector-223876810.jpg</td>\n",
       "      <td>tree</td>\n",
       "      <td>[{'object': 'background', 'predicate': 'on a'}]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>000000000.jpg</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000000000.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>000000011</td>\n",
       "      <td>jpg</td>\n",
       "      <td>festive banner with flags and an inscription .</td>\n",
       "      <td>logo,flag,illustration,red,text,font,emblem,graphic design,graphics,label,symbol,art</td>\n",
       "      <td>festive banner with an inscription, but not with flags</td>\n",
       "      <td>https://thumb1.shutterstock.com/display_pic_with_logo/161878175/475029928/stock-vector-vector-festive-banner-with-flags-of-the-vietnam-and-an-inscription-socialist-republic-of-vietnam-475029928.jpg</td>\n",
       "      <td>banner</td>\n",
       "      <td>[{'object': 'flags', 'predicate': 'with'}, {'object': 'inscription', 'predicate': 'with'}]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>000000011.jpg</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000000011.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000000001</td>\n",
       "      <td>jpg</td>\n",
       "      <td>item : drawing of a figure surrounded by person</td>\n",
       "      <td>drawing,modern art,line,visual arts,art,sketch,artwork,photographic paper,painting,illustration,black-and-white</td>\n",
       "      <td>item : drawing without a figure surrounded by person</td>\n",
       "      <td>https://i.pinimg.com/736x/f9/fd/48/f9fd48780900641ded7ab53d74fe86fe--figure-painting-figure-drawing.jpg</td>\n",
       "      <td>drawing</td>\n",
       "      <td>[{'object': 'figure', 'predicate': 'of a'}, {'object': 'person', 'predicate': 'surrounded by'}]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>000000001.jpg</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000000001.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>000000021</td>\n",
       "      <td>jpg</td>\n",
       "      <td>pop artist performs during the media call</td>\n",
       "      <td>performance,stage,dancer,entertainment,music artist,event,performing arts,singer,performance art,talent show,public event,dance,music venue,concert dance,musician</td>\n",
       "      <td>pop artist performs without the media call</td>\n",
       "      <td>https://media.gettyimages.com/photos/singer-deni-hines-performs-during-the-media-call-for-dusty-the-pop-picture-id57161252?s=612x612</td>\n",
       "      <td>pop artist</td>\n",
       "      <td>[{'object': 'media call', 'predicate': 'during the'}]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>000000021.jpg</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000000021.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>000000024</td>\n",
       "      <td>jpg</td>\n",
       "      <td>actor attends the opening night after party</td>\n",
       "      <td>premiere,carpet,red carpet,bow tie,hat,singer,fashion accessory,performance,tuxedo,formal wear,suit,musician</td>\n",
       "      <td>actor not attending the opening night after party</td>\n",
       "      <td>https://media.gettyimages.com/photos/actor-henry-kelemen-attends-the-our-new-girl-opening-night-after-at-picture-id450417500?s=612x612</td>\n",
       "      <td>actor</td>\n",
       "      <td>[{'object': 'opening night after party', 'predicate': 'attends the'}]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>000000024.jpg</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000000024.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_number file_extension  \\\n",
       "0    000000003            jpg   \n",
       "1    000000005            jpg   \n",
       "2    000000009            jpg   \n",
       "3    000000018            jpg   \n",
       "4    000000007            jpg   \n",
       "5    000000000            jpg   \n",
       "6    000000011            jpg   \n",
       "7    000000001            jpg   \n",
       "8    000000021            jpg   \n",
       "9    000000024            jpg   \n",
       "\n",
       "                                                                  true_caption  \\\n",
       "0                                            actor attends the season premiere   \n",
       "1                                         a woman walks her dog on the beach .   \n",
       "2  close up portrait of a smiling middle aged woman sitting against white wall   \n",
       "3                              man sitting on floor beside a pool using laptop   \n",
       "4                             a beautiful day with some buildings and plants .   \n",
       "5                                       christmas tree on a black background .   \n",
       "6                               festive banner with flags and an inscription .   \n",
       "7                              item : drawing of a figure surrounded by person   \n",
       "8                                    pop artist performs during the media call   \n",
       "9                                  actor attends the opening night after party   \n",
       "\n",
       "                                                                                                                                                               labels  \\\n",
       "0                                                                                                                     musician,premiere,event,singer,suit,performance   \n",
       "1                                                                   water,beach,sea,shore,ocean,canidae,dog,sky,wave,coast,mudflat,dog walking,human,sand,photography   \n",
       "2                                                               hair,sitting,facial expression,nose,arm,cheek,smile,chin,lip,hand,close-up,brown hair,finger,wool,fur   \n",
       "3         sitting,tablet computer,table,technology,grass,drinking,swimming pool,leisure,furniture,gadget,electronic device,vacation,laptop,stock photography,computer   \n",
       "4                             residential area,property,home,building,house,real estate,neighbourhood,town,architecture,estate,suburb,driveway,tree,road surface,road   \n",
       "5           christmas tree,christmas decoration,font,text,graphic design,illustration,interior design,tree,christmas eve,ornament,fir,plant,pine,pine family,graphics   \n",
       "6                                                                                logo,flag,illustration,red,text,font,emblem,graphic design,graphics,label,symbol,art   \n",
       "7                                                     drawing,modern art,line,visual arts,art,sketch,artwork,photographic paper,painting,illustration,black-and-white   \n",
       "8  performance,stage,dancer,entertainment,music artist,event,performing arts,singer,performance art,talent show,public event,dance,music venue,concert dance,musician   \n",
       "9                                                        premiere,carpet,red carpet,bow tie,hat,singer,fashion accessory,performance,tuxedo,formal wear,suit,musician   \n",
       "\n",
       "                                                       negated_caption  \\\n",
       "0                             actor, not attending the season premiere   \n",
       "1                           a woman walks on the beach without her dog   \n",
       "2  close up portrait of a smiling middle aged woman without white wall   \n",
       "3                     man sitting on floor without a pool using laptop   \n",
       "4                                       a beautiful day without plants   \n",
       "5                            christmas tree, not on a black background   \n",
       "6               festive banner with an inscription, but not with flags   \n",
       "7                 item : drawing without a figure surrounded by person   \n",
       "8                           pop artist performs without the media call   \n",
       "9                    actor not attending the opening night after party   \n",
       "\n",
       "                                                                                                                                                                                                     url  \\\n",
       "0                                                               https://media.gettyimages.com/photos/aidan-gillen-attends-the-season-7-premiere-of-hbos-game-of-thrones-at-picture-id817717986?s=612x612   \n",
       "1                                                                              https://media.gettyimages.com/photos/woman-walks-her-dog-on-the-beach-on-october-21-2014-in-saltcoats-picture-id457587968   \n",
       "2                                                           http://l7.alamy.com/zooms/29164f933d7340be90af1ab4c91f3644/close-up-portrait-of-a-smiling-middle-aged-woman-sitting-against-white-hxeygn.jpg   \n",
       "3                                                                                     https://media.gettyimages.com/photos/man-sitting-on-floor-beside-a-pool-using-laptop-picture-id588490995?s=612x612   \n",
       "4                                                                       https://d1tq208oegmb9e.cloudfront.net/site_photos_image/dbx%3A/urban+project/orange+county/fullerton/chapman+villas/Photos/3.jpg   \n",
       "5                                                          https://thumb1.shutterstock.com/display_pic_with_logo/261388/223876810/stock-vector-christmas-tree-on-a-black-background-vector-223876810.jpg   \n",
       "6  https://thumb1.shutterstock.com/display_pic_with_logo/161878175/475029928/stock-vector-vector-festive-banner-with-flags-of-the-vietnam-and-an-inscription-socialist-republic-of-vietnam-475029928.jpg   \n",
       "7                                                                                                https://i.pinimg.com/736x/f9/fd/48/f9fd48780900641ded7ab53d74fe86fe--figure-painting-figure-drawing.jpg   \n",
       "8                                                                   https://media.gettyimages.com/photos/singer-deni-hines-performs-during-the-media-call-for-dusty-the-pop-picture-id57161252?s=612x612   \n",
       "9                                                                 https://media.gettyimages.com/photos/actor-henry-kelemen-attends-the-our-new-girl-opening-night-after-at-picture-id450417500?s=612x612   \n",
       "\n",
       "      subject  \\\n",
       "0       actor   \n",
       "1       woman   \n",
       "2    portrait   \n",
       "3         man   \n",
       "4         day   \n",
       "5        tree   \n",
       "6      banner   \n",
       "7     drawing   \n",
       "8  pop artist   \n",
       "9       actor   \n",
       "\n",
       "                                                                                                                                                                object_predicate_pairs  \\\n",
       "0                                                                                                                          [{'object': 'season premiere', 'predicate': 'attends the'}]   \n",
       "1                                                                                                                           [{'object': 'dog', 'predicate': 'walks her on the beach'}]   \n",
       "2  [{'object': 'woman', 'predicate': 'of a'}, {'object': 'wall', 'predicate': 'against white'}, {'object': 'age', 'predicate': 'middle'}, {'object': 'smile', 'predicate': 'smiling'}]   \n",
       "3                                                    [{'object': 'floor', 'predicate': 'on'}, {'object': 'pool', 'predicate': 'beside a'}, {'object': 'laptop', 'predicate': 'using'}]   \n",
       "4                                                                                  [{'object': 'buildings', 'predicate': 'with some'}, {'object': 'plants', 'predicate': 'with some'}]   \n",
       "5                                                                                                                                      [{'object': 'background', 'predicate': 'on a'}]   \n",
       "6                                                                                           [{'object': 'flags', 'predicate': 'with'}, {'object': 'inscription', 'predicate': 'with'}]   \n",
       "7                                                                                      [{'object': 'figure', 'predicate': 'of a'}, {'object': 'person', 'predicate': 'surrounded by'}]   \n",
       "8                                                                                                                                [{'object': 'media call', 'predicate': 'during the'}]   \n",
       "9                                                                                                                [{'object': 'opening night after party', 'predicate': 'attends the'}]   \n",
       "\n",
       "  predicate  negate_word_present  num_ops     image_name  \\\n",
       "0      None                 True        1  000000003.jpg   \n",
       "1      None                 True        1  000000005.jpg   \n",
       "2      None                 True        4  000000009.jpg   \n",
       "3      None                 True        3  000000018.jpg   \n",
       "4      None                 True        2  000000007.jpg   \n",
       "5      None                 True        1  000000000.jpg   \n",
       "6      None                 True        2  000000011.jpg   \n",
       "7      None                 True        2  000000001.jpg   \n",
       "8      None                 True        1  000000021.jpg   \n",
       "9      None                 True        1  000000024.jpg   \n",
       "\n",
       "                                                                                                                  image_path  \n",
       "0  /Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000000003.jpg  \n",
       "1  /Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000000005.jpg  \n",
       "2  /Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000000009.jpg  \n",
       "3  /Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000000018.jpg  \n",
       "4  /Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000000007.jpg  \n",
       "5  /Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000000000.jpg  \n",
       "6  /Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000000011.jpg  \n",
       "7  /Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000000001.jpg  \n",
       "8  /Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000000021.jpg  \n",
       "9  /Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000000024.jpg  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotiation_df['image_name'] = annotiation_df['image_number'] + '.' + annotiation_df['file_extension']\n",
    "\n",
    "data_df = pd.merge(annotiation_df, images_metadata_df, on='image_name', how='inner')\n",
    "print(data_df.shape)\n",
    "data_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split this dataset into different negation types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_caption(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)  # Remove punctuation\n",
    "        text = text.strip()\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "data_df['true_caption_clean'] = data_df['true_caption'].apply(clean_caption)\n",
    "data_df['negated_caption_clean'] = data_df['negated_caption'].apply(clean_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_number</th>\n",
       "      <th>file_extension</th>\n",
       "      <th>true_caption</th>\n",
       "      <th>labels</th>\n",
       "      <th>negated_caption</th>\n",
       "      <th>url</th>\n",
       "      <th>subject</th>\n",
       "      <th>object_predicate_pairs</th>\n",
       "      <th>predicate</th>\n",
       "      <th>negate_word_present</th>\n",
       "      <th>num_ops</th>\n",
       "      <th>image_name</th>\n",
       "      <th>image_path</th>\n",
       "      <th>true_caption_clean</th>\n",
       "      <th>negated_caption_clean</th>\n",
       "      <th>negation_bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000003</td>\n",
       "      <td>jpg</td>\n",
       "      <td>actor attends the season premiere</td>\n",
       "      <td>musician,premiere,event,singer,suit,performance</td>\n",
       "      <td>actor, not attending the season premiere</td>\n",
       "      <td>https://media.gettyimages.com/photos/aidan-gillen-attends-the-season-7-premiere-of-hbos-game-of-thrones-at-picture-id817717986?s=612x612</td>\n",
       "      <td>actor</td>\n",
       "      <td>[{'object': 'season premiere', 'predicate': 'attends the'}]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>000000003.jpg</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000000003.jpg</td>\n",
       "      <td>actor attends the season premiere</td>\n",
       "      <td>actor  not attending the season premiere</td>\n",
       "      <td>syntactic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000000005</td>\n",
       "      <td>jpg</td>\n",
       "      <td>a woman walks her dog on the beach .</td>\n",
       "      <td>water,beach,sea,shore,ocean,canidae,dog,sky,wave,coast,mudflat,dog walking,human,sand,photography</td>\n",
       "      <td>a woman walks on the beach without her dog</td>\n",
       "      <td>https://media.gettyimages.com/photos/woman-walks-her-dog-on-the-beach-on-october-21-2014-in-saltcoats-picture-id457587968</td>\n",
       "      <td>woman</td>\n",
       "      <td>[{'object': 'dog', 'predicate': 'walks her on the beach'}]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>000000005.jpg</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000000005.jpg</td>\n",
       "      <td>a woman walks her dog on the beach</td>\n",
       "      <td>a woman walks on the beach without her dog</td>\n",
       "      <td>lexical/semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000000009</td>\n",
       "      <td>jpg</td>\n",
       "      <td>close up portrait of a smiling middle aged woman sitting against white wall</td>\n",
       "      <td>hair,sitting,facial expression,nose,arm,cheek,smile,chin,lip,hand,close-up,brown hair,finger,wool,fur</td>\n",
       "      <td>close up portrait of a smiling middle aged woman without white wall</td>\n",
       "      <td>http://l7.alamy.com/zooms/29164f933d7340be90af1ab4c91f3644/close-up-portrait-of-a-smiling-middle-aged-woman-sitting-against-white-hxeygn.jpg</td>\n",
       "      <td>portrait</td>\n",
       "      <td>[{'object': 'woman', 'predicate': 'of a'}, {'object': 'wall', 'predicate': 'against white'}, {'object': 'age', 'predicate': 'middle'}, {'object': 'smile', 'predicate': 'smiling'}]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>000000009.jpg</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000000009.jpg</td>\n",
       "      <td>close up portrait of a smiling middle aged woman sitting against white wall</td>\n",
       "      <td>close up portrait of a smiling middle aged woman without white wall</td>\n",
       "      <td>lexical/semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000000018</td>\n",
       "      <td>jpg</td>\n",
       "      <td>man sitting on floor beside a pool using laptop</td>\n",
       "      <td>sitting,tablet computer,table,technology,grass,drinking,swimming pool,leisure,furniture,gadget,electronic device,vacation,laptop,stock photography,computer</td>\n",
       "      <td>man sitting on floor without a pool using laptop</td>\n",
       "      <td>https://media.gettyimages.com/photos/man-sitting-on-floor-beside-a-pool-using-laptop-picture-id588490995?s=612x612</td>\n",
       "      <td>man</td>\n",
       "      <td>[{'object': 'floor', 'predicate': 'on'}, {'object': 'pool', 'predicate': 'beside a'}, {'object': 'laptop', 'predicate': 'using'}]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>000000018.jpg</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000000018.jpg</td>\n",
       "      <td>man sitting on floor beside a pool using laptop</td>\n",
       "      <td>man sitting on floor without a pool using laptop</td>\n",
       "      <td>lexical/semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000000007</td>\n",
       "      <td>jpg</td>\n",
       "      <td>a beautiful day with some buildings and plants .</td>\n",
       "      <td>residential area,property,home,building,house,real estate,neighbourhood,town,architecture,estate,suburb,driveway,tree,road surface,road</td>\n",
       "      <td>a beautiful day without plants</td>\n",
       "      <td>https://d1tq208oegmb9e.cloudfront.net/site_photos_image/dbx%3A/urban+project/orange+county/fullerton/chapman+villas/Photos/3.jpg</td>\n",
       "      <td>day</td>\n",
       "      <td>[{'object': 'buildings', 'predicate': 'with some'}, {'object': 'plants', 'predicate': 'with some'}]</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>000000007.jpg</td>\n",
       "      <td>/Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000000007.jpg</td>\n",
       "      <td>a beautiful day with some buildings and plants</td>\n",
       "      <td>a beautiful day without plants</td>\n",
       "      <td>lexical/semantic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_number file_extension  \\\n",
       "0    000000003            jpg   \n",
       "1    000000005            jpg   \n",
       "2    000000009            jpg   \n",
       "3    000000018            jpg   \n",
       "4    000000007            jpg   \n",
       "\n",
       "                                                                  true_caption  \\\n",
       "0                                            actor attends the season premiere   \n",
       "1                                         a woman walks her dog on the beach .   \n",
       "2  close up portrait of a smiling middle aged woman sitting against white wall   \n",
       "3                              man sitting on floor beside a pool using laptop   \n",
       "4                             a beautiful day with some buildings and plants .   \n",
       "\n",
       "                                                                                                                                                        labels  \\\n",
       "0                                                                                                              musician,premiere,event,singer,suit,performance   \n",
       "1                                                            water,beach,sea,shore,ocean,canidae,dog,sky,wave,coast,mudflat,dog walking,human,sand,photography   \n",
       "2                                                        hair,sitting,facial expression,nose,arm,cheek,smile,chin,lip,hand,close-up,brown hair,finger,wool,fur   \n",
       "3  sitting,tablet computer,table,technology,grass,drinking,swimming pool,leisure,furniture,gadget,electronic device,vacation,laptop,stock photography,computer   \n",
       "4                      residential area,property,home,building,house,real estate,neighbourhood,town,architecture,estate,suburb,driveway,tree,road surface,road   \n",
       "\n",
       "                                                       negated_caption  \\\n",
       "0                             actor, not attending the season premiere   \n",
       "1                           a woman walks on the beach without her dog   \n",
       "2  close up portrait of a smiling middle aged woman without white wall   \n",
       "3                     man sitting on floor without a pool using laptop   \n",
       "4                                       a beautiful day without plants   \n",
       "\n",
       "                                                                                                                                            url  \\\n",
       "0      https://media.gettyimages.com/photos/aidan-gillen-attends-the-season-7-premiere-of-hbos-game-of-thrones-at-picture-id817717986?s=612x612   \n",
       "1                     https://media.gettyimages.com/photos/woman-walks-her-dog-on-the-beach-on-october-21-2014-in-saltcoats-picture-id457587968   \n",
       "2  http://l7.alamy.com/zooms/29164f933d7340be90af1ab4c91f3644/close-up-portrait-of-a-smiling-middle-aged-woman-sitting-against-white-hxeygn.jpg   \n",
       "3                            https://media.gettyimages.com/photos/man-sitting-on-floor-beside-a-pool-using-laptop-picture-id588490995?s=612x612   \n",
       "4              https://d1tq208oegmb9e.cloudfront.net/site_photos_image/dbx%3A/urban+project/orange+county/fullerton/chapman+villas/Photos/3.jpg   \n",
       "\n",
       "    subject  \\\n",
       "0     actor   \n",
       "1     woman   \n",
       "2  portrait   \n",
       "3       man   \n",
       "4       day   \n",
       "\n",
       "                                                                                                                                                                object_predicate_pairs  \\\n",
       "0                                                                                                                          [{'object': 'season premiere', 'predicate': 'attends the'}]   \n",
       "1                                                                                                                           [{'object': 'dog', 'predicate': 'walks her on the beach'}]   \n",
       "2  [{'object': 'woman', 'predicate': 'of a'}, {'object': 'wall', 'predicate': 'against white'}, {'object': 'age', 'predicate': 'middle'}, {'object': 'smile', 'predicate': 'smiling'}]   \n",
       "3                                                    [{'object': 'floor', 'predicate': 'on'}, {'object': 'pool', 'predicate': 'beside a'}, {'object': 'laptop', 'predicate': 'using'}]   \n",
       "4                                                                                  [{'object': 'buildings', 'predicate': 'with some'}, {'object': 'plants', 'predicate': 'with some'}]   \n",
       "\n",
       "  predicate  negate_word_present  num_ops     image_name  \\\n",
       "0      None                 True        1  000000003.jpg   \n",
       "1      None                 True        1  000000005.jpg   \n",
       "2      None                 True        4  000000009.jpg   \n",
       "3      None                 True        3  000000018.jpg   \n",
       "4      None                 True        2  000000007.jpg   \n",
       "\n",
       "                                                                                                                  image_path  \\\n",
       "0  /Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000000003.jpg   \n",
       "1  /Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000000005.jpg   \n",
       "2  /Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000000009.jpg   \n",
       "3  /Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000000018.jpg   \n",
       "4  /Users/akanshagautam/Documents/MTech/Thesis/Dataset/ConClip/ccneg_images/cc3m_subset_images_extracted_final/000000007.jpg   \n",
       "\n",
       "                                                            true_caption_clean  \\\n",
       "0                                            actor attends the season premiere   \n",
       "1                                           a woman walks her dog on the beach   \n",
       "2  close up portrait of a smiling middle aged woman sitting against white wall   \n",
       "3                              man sitting on floor beside a pool using laptop   \n",
       "4                               a beautiful day with some buildings and plants   \n",
       "\n",
       "                                                 negated_caption_clean  \\\n",
       "0                             actor  not attending the season premiere   \n",
       "1                           a woman walks on the beach without her dog   \n",
       "2  close up portrait of a smiling middle aged woman without white wall   \n",
       "3                     man sitting on floor without a pool using laptop   \n",
       "4                                       a beautiful day without plants   \n",
       "\n",
       "    negation_bucket  \n",
       "0         syntactic  \n",
       "1  lexical/semantic  \n",
       "2  lexical/semantic  \n",
       "3  lexical/semantic  \n",
       "4  lexical/semantic  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syntactic_negators = {\"no\", \"not\", \"never\", \"neither\", \"nobody\", \"nothing\"}\n",
    "\n",
    "def is_syntactic_negation(caption):\n",
    "    tokens = caption.lower().split()\n",
    "    return any(word in syntactic_negators for word in tokens)\n",
    "\n",
    "def is_morphological_negation(caption):\n",
    "    # Basic prefix patterns for morphological negation\n",
    "    prefixes = (\"un\", \"dis\", \"in\", \"im\", \"ir\", \"il\", \"non\", \"mis\")\n",
    "    words = caption.lower().split()\n",
    "    return any(word.startswith(prefix) for word in words for prefix in prefixes)\n",
    "\n",
    "lexical_negators = {\"without\", \"lack\", \"absent\", \"avoid\", \"missing\"}\n",
    "\n",
    "def is_lexical_negation(caption):\n",
    "    tokens = caption.lower().split()\n",
    "    return any(word in lexical_negators for word in tokens)\n",
    "\n",
    "def classify_negation(caption):\n",
    "    if is_syntactic_negation(caption):\n",
    "        return \"syntactic\"\n",
    "    elif is_lexical_negation(caption):\n",
    "        return \"lexical/semantic\"\n",
    "    elif is_morphological_negation(caption):\n",
    "        return \"morphological\"\n",
    "    else:\n",
    "        return \"unknown/pragmatic\"\n",
    "    \n",
    "data_df[\"negation_bucket\"] = data_df[\"negated_caption_clean\"].apply(classify_negation)\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buckets: ['syntactic' 'lexical/semantic']\n",
      "Syntactic DF: (150735, 16)\n",
      "Lexical DF: (77511, 16)\n"
     ]
    }
   ],
   "source": [
    "buckets = data_df[\"negation_bucket\"].unique()\n",
    "print(f\"Buckets: {buckets}\")\n",
    "\n",
    "bucket_datasets = {}\n",
    "\n",
    "for bucket in buckets:\n",
    "    bucket_datasets[bucket] = data_df[data_df[\"negation_bucket\"] == bucket]\n",
    "\n",
    "syntactic_df = bucket_datasets.get(\"syntactic\")\n",
    "print(f\"Syntactic DF: {syntactic_df.shape}\")\n",
    "\n",
    "lexical_df = bucket_datasets.get(\"lexical/semantic\")\n",
    "print(f\"Lexical DF: {lexical_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_negation_sensitivity(df, model, processor):\n",
    "    \"\"\"\n",
    "    Evaluate the negation sensitivity score for a given dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing 'image_path', 'true_caption', and 'negated_caption'.\n",
    "        model: The vision-language model for embedding generation.\n",
    "        processor: The corresponding processor to preprocess data.\n",
    "\n",
    "    Returns:\n",
    "        results_df (pd.DataFrame): DataFrame with added columns for similarity and comparison.\n",
    "        negation_sensitivity_score (float): Final computed score.\n",
    "    \"\"\"\n",
    "    # Prepare lists to store results\n",
    "    similarity_true_list = []\n",
    "    similarity_neg_list = []\n",
    "    comparison_list = []\n",
    "\n",
    "    # Iterate through each row in the dataframe with tqdm\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Evaluating rows\"):\n",
    "        image_path = row[\"image_path\"]\n",
    "        true_caption = row[\"true_caption\"]\n",
    "        negated_caption = row[\"negated_caption\"]\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Evaluate true caption\n",
    "        inputs_true = processor(text=[true_caption], images=image, return_tensors=\"pt\", padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs_true = model(**inputs_true)\n",
    "            similarity_true = torch.nn.functional.cosine_similarity(\n",
    "                outputs_true.image_embeds, outputs_true.text_embeds\n",
    "            ).item()\n",
    "\n",
    "        # Evaluate negated caption\n",
    "        inputs_neg = processor(text=[negated_caption], images=image, return_tensors=\"pt\", padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs_neg = model(**inputs_neg)\n",
    "            similarity_neg = torch.nn.functional.cosine_similarity(\n",
    "                outputs_neg.image_embeds, outputs_neg.text_embeds\n",
    "            ).item()\n",
    "\n",
    "        # Store results\n",
    "        similarity_true_list.append(similarity_true)\n",
    "        similarity_neg_list.append(similarity_neg)\n",
    "        comparison_list.append(int(similarity_true > similarity_neg))\n",
    "\n",
    "    # Create a new dataframe with results\n",
    "    results_df = df.copy()\n",
    "    results_df[\"similarity_true\"] = similarity_true_list\n",
    "    results_df[\"similarity_neg\"] = similarity_neg_list\n",
    "    results_df[\"true_greater_than_negated\"] = comparison_list\n",
    "\n",
    "    # Compute the final evaluation metric\n",
    "    negation_sensitivity_score = sum(comparison_list) / len(comparison_list)\n",
    "\n",
    "    return results_df, negation_sensitivity_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Clip ViT Large Patch14 @336px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akanshagautam/Documents/MTech/ml_env/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14-336\" )\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating rows:   0%|          | 50/150735 [01:37<81:21:00,  1.94s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results_df, score \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_negation_sensitivity\u001b[49m\u001b[43m(\u001b[49m\u001b[43msyntactic_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNegation Sensitivity Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m results_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "Cell \u001b[0;32mIn[13], line 39\u001b[0m, in \u001b[0;36mevaluate_negation_sensitivity\u001b[0;34m(df, model, processor)\u001b[0m\n\u001b[1;32m     37\u001b[0m inputs_neg \u001b[38;5;241m=\u001b[39m processor(text\u001b[38;5;241m=\u001b[39m[negated_caption], images\u001b[38;5;241m=\u001b[39mimage, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 39\u001b[0m     outputs_neg \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs_neg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     similarity_neg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcosine_similarity(\n\u001b[1;32m     41\u001b[0m         outputs_neg\u001b[38;5;241m.\u001b[39mimage_embeds, outputs_neg\u001b[38;5;241m.\u001b[39mtext_embeds\n\u001b[1;32m     42\u001b[0m     )\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Store results\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MTech/ml_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1749\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MTech/ml_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1760\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1756\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1758\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1759\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1762\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1763\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/MTech/ml_env/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py:1125\u001b[0m, in \u001b[0;36mCLIPModel.forward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, return_loss, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1120\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1121\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m   1122\u001b[0m )\n\u001b[1;32m   1123\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1125\u001b[0m vision_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m text_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_model(\n\u001b[1;32m   1133\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1134\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1139\u001b[0m )\n\u001b[1;32m   1141\u001b[0m image_embeds \u001b[38;5;241m=\u001b[39m vision_outputs[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/MTech/ml_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1749\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MTech/ml_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1760\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1756\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1758\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1759\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1762\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1763\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/MTech/ml_env/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py:869\u001b[0m, in \u001b[0;36mCLIPVisionTransformer.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    866\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values)\n\u001b[1;32m    867\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_layrnorm(hidden_states)\n\u001b[0;32m--> 869\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    876\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    877\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m last_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]\n",
      "File \u001b[0;32m~/Documents/MTech/ml_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1749\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MTech/ml_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1760\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1756\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1758\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1759\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1762\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1763\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/MTech/ml_env/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py:655\u001b[0m, in \u001b[0;36mCLIPEncoder.forward\u001b[0;34m(self, inputs_embeds, attention_mask, causal_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    648\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    649\u001b[0m         create_custom_forward(encoder_layer),\n\u001b[1;32m    650\u001b[0m         hidden_states,\n\u001b[1;32m    651\u001b[0m         attention_mask,\n\u001b[1;32m    652\u001b[0m         causal_attention_mask,\n\u001b[1;32m    653\u001b[0m     )\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 655\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    662\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/Documents/MTech/ml_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1749\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MTech/ml_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1760\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1756\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1758\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1759\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1762\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1763\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/MTech/ml_env/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py:385\u001b[0m, in \u001b[0;36mCLIPEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    382\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    384\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm1(hidden_states)\n\u001b[0;32m--> 385\u001b[0m hidden_states, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    393\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/Documents/MTech/ml_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1749\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MTech/ml_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1760\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1756\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1758\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1759\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1762\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1763\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/MTech/ml_env/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py:310\u001b[0m, in \u001b[0;36mCLIPAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    307\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m attn_weights\u001b[38;5;241m.\u001b[39mview(bsz, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, tgt_len, src_len) \u001b[38;5;241m+\u001b[39m attention_mask\n\u001b[1;32m    308\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m attn_weights\u001b[38;5;241m.\u001b[39mview(bsz \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, tgt_len, src_len)\n\u001b[0;32m--> 310\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;66;03m# this operation is a bit akward, but it's required to\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;66;03m# make sure that attn_weights keeps its gradient.\u001b[39;00m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;66;03m# In order to do so, attn_weights have to reshaped\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;66;03m# twice and have to be reused in the following\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     attn_weights_reshaped \u001b[38;5;241m=\u001b[39m attn_weights\u001b[38;5;241m.\u001b[39mview(bsz, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, tgt_len, src_len)\n",
      "File \u001b[0;32m~/Documents/MTech/ml_env/lib/python3.9/site-packages/torch/nn/functional.py:2140\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   2138\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   2139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2140\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2142\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results_df, score = evaluate_negation_sensitivity(syntactic_df, model, processor)\n",
    "print(f\"\\nNegation Sensitivity Score: {score:.4f}\")\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df, score = evaluate_negation_sensitivity(lexical_df, model, processor)\n",
    "print(f\"\\nNegation Sensitivity Score: {score:.4f}\")\n",
    "results_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
