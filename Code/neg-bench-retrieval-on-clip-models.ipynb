{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akanshagautam/Documents/MTech/Thesis/mtp_env/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/akanshagautam/Documents/MTech/Thesis/mtp_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import re\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"max_colwidth\", None)\n",
    "from tqdm import tqdm\n",
    "import clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load images metadata to a df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dir = \"/workspace/Dataset/conclip/ccneg_images/cc3m_subset_images_extracted_final\"\n",
    "\n",
    "image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff', '.webp'}\n",
    "\n",
    "image_data = []\n",
    "for root, _, files in os.walk(image_dir):\n",
    "    for file in files:\n",
    "        if os.path.splitext(file)[1].lower() in image_extensions:\n",
    "            image_path = os.path.join(root, file)\n",
    "            image_data.append({'image_name': file, 'image_path': image_path})\n",
    "\n",
    "images_metadata_df = pd.DataFrame(image_data)\n",
    "print(images_metadata_df.shape)\n",
    "images_metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the CLIP model\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Define path to image dataset\n",
    "image_dir = \"/path/to/your/images\"  # e.g., COCO or VOC2007\n",
    "image_files = [os.path.join(image_dir, fname) for fname in os.listdir(image_dir)\n",
    "               if fname.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "# Load and preprocess images\n",
    "image_embeddings = []\n",
    "image_paths = []\n",
    "print(\"Encoding images...\")\n",
    "for img_path in tqdm(image_files):\n",
    "    try:\n",
    "        image = preprocess(Image.open(img_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            image_feat = model.encode_image(image)\n",
    "        image_embeddings.append(image_feat.cpu())\n",
    "        image_paths.append(img_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {img_path}: {e}\")\n",
    "\n",
    "# Stack image features\n",
    "image_embeddings = torch.cat(image_embeddings, dim=0)\n",
    "image_embeddings /= image_embeddings.norm(dim=-1, keepdim=True)  # normalize\n",
    "\n",
    "# Define a negated text query\n",
    "query = \"a dog without a leash\"  # replace with your own\n",
    "text_tokens = clip.tokenize([query]).to(device)\n",
    "with torch.no_grad():\n",
    "    text_embedding = model.encode_text(text_tokens).cpu()\n",
    "    text_embedding /= text_embedding.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Compute cosine similarity\n",
    "similarities = (100.0 * image_embeddings @ text_embedding.T).squeeze(1)\n",
    "\n",
    "# Rank and display top-k\n",
    "top_k = 5\n",
    "top_indices = similarities.topk(top_k).indices\n",
    "print(f\"\\nTop {top_k} results for query: '{query}'\\n\")\n",
    "\n",
    "# Display top results\n",
    "for rank, idx in enumerate(top_indices):\n",
    "    path = image_paths[idx]\n",
    "    score = similarities[idx].item()\n",
    "    print(f\"Rank {rank + 1}: {path} (Score: {score:.2f})\")\n",
    "    img = Image.open(path)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Rank {rank + 1} | Score: {score:.2f}\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
